p8105\_hw5\_niz2000
================
Nora Zakaria
11/11/2019

# Problem 1

Problem 1 loads the Iris dataset from the tidyverse package, and
includes missing values in each
    column.

## Load Tidyverse and the Iris Data

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.2.1     ✔ purrr   0.3.2
    ## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
    ## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
    ## ✔ readr   1.3.1     ✔ forcats 0.4.0

    ## ── Conflicts ────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
iris_with_missing
```

    ## # A tibble: 150 x 5
    ##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    ##           <dbl>       <dbl>        <dbl>       <dbl> <chr>  
    ##  1          5.1         3.5          1.4         0.2 setosa 
    ##  2          4.9         3            1.4         0.2 setosa 
    ##  3          4.7         3.2          1.3         0.2 setosa 
    ##  4          4.6         3.1          1.5        NA   setosa 
    ##  5          5           3.6          1.4         0.2 setosa 
    ##  6          5.4         3.9          1.7         0.4 setosa 
    ##  7         NA           3.4          1.4         0.3 setosa 
    ##  8          5           3.4          1.5         0.2 setosa 
    ##  9          4.4         2.9          1.4         0.2 setosa 
    ## 10          4.9         3.1         NA           0.1 setosa 
    ## # … with 140 more rows

Each of the 5 variables, “Sepal.Length,” “Sepal.Width,” “Petal.Length,”
“Petal.Width,” and “Species” contain missing values, indicated by an
NA. The descriptive variable names were preserved, and the “Species”
variable was converted to a character variable.

## Function to Replace Missing Values

This function takes a vector as an argument, and replaces all missing
values from the uploaded Iris dataset titled “iris\_with\_missing,” and
returns a resulting vector titled “iris\_complete.”

``` r
replace_missing = function(x) {
  if (is.numeric(x)) {
    ifelse(is.na(x), mean(x, na.rm = TRUE), x)
  } else if (is.character(x)) {
    ifelse(is.na(x), "virginica", x)
  }
}
iris_complete = map_df(iris_with_missing, replace_missing)
iris_complete
```

    ## # A tibble: 150 x 5
    ##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    ##           <dbl>       <dbl>        <dbl>       <dbl> <chr>  
    ##  1         5.1          3.5         1.4         0.2  setosa 
    ##  2         4.9          3           1.4         0.2  setosa 
    ##  3         4.7          3.2         1.3         0.2  setosa 
    ##  4         4.6          3.1         1.5         1.19 setosa 
    ##  5         5            3.6         1.4         0.2  setosa 
    ##  6         5.4          3.9         1.7         0.4  setosa 
    ##  7         5.82         3.4         1.4         0.3  setosa 
    ##  8         5            3.4         1.5         0.2  setosa 
    ##  9         4.4          2.9         1.4         0.2  setosa 
    ## 10         4.9          3.1         3.77        0.1  setosa 
    ## # … with 140 more rows

Missing values for each numeric variable “Sepal.Length,” “Sepal.Width,”
“Petal.Length,” and “Petal.Width” were replaced with the mean of the
non-missing values for that particular variable. For the character
variable “Species,” all missing values were replaced with the name
“virginica.” The map statement was also used in the function to apply
the function to the all of the columns.

# Problem 2

Problem 2 loads a zipfile containing data from a longitudinal study,
including data from a control arm and an experimental arm. Data for each
participant is included in a separate file, and file names include the
subject ID and abbreviation of the arm, “con” or “exp.”

## Load in the Zipfile Data

``` r
data_df = tibble(
  participant = list.files(path = "./data")
)
data_df
```

    ## # A tibble: 20 x 1
    ##    participant
    ##    <chr>      
    ##  1 con_01.csv 
    ##  2 con_02.csv 
    ##  3 con_03.csv 
    ##  4 con_04.csv 
    ##  5 con_05.csv 
    ##  6 con_06.csv 
    ##  7 con_07.csv 
    ##  8 con_08.csv 
    ##  9 con_09.csv 
    ## 10 con_10.csv 
    ## 11 exp_01.csv 
    ## 12 exp_02.csv 
    ## 13 exp_03.csv 
    ## 14 exp_04.csv 
    ## 15 exp_05.csv 
    ## 16 exp_06.csv 
    ## 17 exp_07.csv 
    ## 18 exp_08.csv 
    ## 19 exp_09.csv 
    ## 20 exp_10.csv

Loading the zipfile data resulted in a dataframe that contains all file
names, using the list.files function.

## Iterate over Files Names

This next step creates a tidy dataframe containing data from all study
participants, including their subject ID, whether they were in the
control or experimental treatment arm, and their weekly observations
over time.

``` r
data_df = function(path){
  case_control_df = read_csv(path)
  case_control_df 
}
directory = "./data"
study_df = tibble(
  observation = list.files(path = "./data"),
    files = str_c(directory, "/",observation)) %>%
  mutate(
    weekly_data = map(files, read_csv),
    observation = str_replace(observation, ".csv","")) %>%
  unnest() %>%
  mutate(
   study_arm = if_else(observation %in% c("con_01", "con_02", "con_03", "con_04", "con_05",
        "con_06", "con_07", "con_08", "con_09", "con_10"), "control", observation),
    study_arm = if_else(study_arm %in% c("exp_01", "exp_02", "exp_03", "exp_04", "exp_05",
        "exp_06", "exp_07", "exp_08", "exp_09", "exp_10"), "experimental", study_arm)) %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "weekly_data") %>%
  mutate(week = str_replace(week, "week_", "")) %>%
  rename(participant_id = observation)
study_df
```

    ## # A tibble: 160 x 5
    ##    participant_id files             study_arm week  weekly_data
    ##    <chr>          <chr>             <chr>     <chr>       <dbl>
    ##  1 con_01         ./data/con_01.csv control   1            0.2 
    ##  2 con_01         ./data/con_01.csv control   2           -1.31
    ##  3 con_01         ./data/con_01.csv control   3            0.66
    ##  4 con_01         ./data/con_01.csv control   4            1.96
    ##  5 con_01         ./data/con_01.csv control   5            0.23
    ##  6 con_01         ./data/con_01.csv control   6            1.09
    ##  7 con_01         ./data/con_01.csv control   7            0.05
    ##  8 con_01         ./data/con_01.csv control   8            1.94
    ##  9 con_02         ./data/con_02.csv control   1            1.13
    ## 10 con_02         ./data/con_02.csv control   2           -0.88
    ## # … with 150 more rows

“Map” was used to iterate over file names and read in data for each
subject, resulting in a new variable called “files” in the dataframe
“study\_df.” The results were unnested, displaying the 8 weeks of data
collected for each study participant. To tidy the data:

  - The “observation” variable values dropped the “.csv” to display only
    the study arm abbreviation and participant number.
  - A new variable “study\_arm” was created to indicate control or
    experimental arm.
  - The data were pivoted longer in order to display the week and the
    data collected that week, for each observation.
  - The “observation” variable was renamed “participant\_id” to be more
    descriptive.

## Spaghetti Plot of Observations for Each Subject Over Time

The spaghetti plot demonstrates weekly data collected on each subject
over time.

``` r
study_df %>% 
  group_by(participant_id, week) %>% 
  ggplot(aes(x = week, y = weekly_data, group = participant_id, color = study_arm)) +
  geom_line() +
  labs(caption = "Observations for Each Subject Over Time") + 
  viridis::scale_color_viridis(
    discrete = TRUE)
```

![](p8105_hw5_niz2000_files/figure-gfm/Spaghetti%20Plot-1.png)<!-- -->

Among the control arm, data for each control participant is variable
week to week, but over the study period there does not appear to be a
trend of a net increase or net decrease in weekly\_data. Among the
experimental arm, weekly data for each experimental participant is
variable, but there does appear to be a net increase in weekly\_data
over time. Overall, the expermintal group has higher weekly\_data
compared to the control group, with overlap among the treatment arm
groups in the middle of the distribution. By week 8 of the study, all
observations in the treatment arm have higher data than all of the
observations in the control arm.

# Problem 3

Problem 3 delves into effect size and whether a false null hypothesis
will be rejected. The probability that a false null hypothesis is
rejected is referred to as power, and depends on several factors
including: the sample size, effect size, and the error variance.

## Set the Regression Design Elements

In order to conduct a simulation to explore the relationship between
power and effect size in a simple linear regression, certain design
elements were set: n=30, xi1 draws from a standard Normal distribution,
β0=2, and σ2=50.

``` r
linear_regression= function(n = 30, beta0 = 2, beta1 = 0) {
  regression_data = tibble(
    x = rnorm(n, mean = 0, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 50^0.5)
  )
  ls_fit = lm(y ~ x, data = regression_data) 
  broom::tidy(ls_fit) %>%
    filter(term == "x")
}

#Function for Beta 1 = 0
  regression_results = rerun(10000, linear_regression()) %>%
    bind_rows()
  
#Function for Beta 1 = 0 through 6
  results = tibble(
    beta1 = c(0, 1, 2, 3, 4, 5, 6)) %>%
    mutate(
      output_list = map(.x = beta1, ~rerun(10000, linear_regression(beta1 = .x))),
      estimate_dfs = map(output_list, bind_rows)) %>%
    select(-output_list) %>%
    unnest(estimate_dfs)
results
```

    ## # A tibble: 70,000 x 6
    ##    beta1 term  estimate std.error statistic p.value
    ##    <dbl> <chr>    <dbl>     <dbl>     <dbl>   <dbl>
    ##  1     0 x       -0.912      1.31    -0.698  0.491 
    ##  2     0 x        2.11       1.10     1.91   0.0665
    ##  3     0 x        0.813      1.12     0.727  0.473 
    ##  4     0 x        2.03       1.24     1.64   0.113 
    ##  5     0 x        1.06       1.02     1.04   0.307 
    ##  6     0 x       -1.48       1.12    -1.32   0.196 
    ##  7     0 x        0.670      1.17     0.571  0.573 
    ##  8     0 x       -1.24       1.63    -0.764  0.451 
    ##  9     0 x        0.223      1.42     0.157  0.876 
    ## 10     0 x        0.221      1.21     0.183  0.856 
    ## # … with 69,990 more rows

The simulation generates 10,000 datasets from the model. Relevant
variables for future analysis includes the “beta1” variable (the true
value), “estimate” variable (the estimate value), and the “p.value”
variable.

## Plot Showing the Proportion of Times the Null was Rejected

This plot displays the power of the test, based on the proportion of
times the null was rejected, and the true value of β1 on the x axis. The
x-axis demonstrates the true β1 value, or the effect size, and the
y-axis demonstrates the proportion of times that the null was rejected,
or the power.

``` r
results %>%
  mutate(rejection = cut(p.value, breaks=c(0, 0.05, Inf), 
    labels=c("Rejected", "Not_Rejected"))) %>%
  group_by(beta1, rejection) %>%
  summarise(count = n()) %>%
  mutate(
    proportion = count / sum(count),
    as.character(beta1)) %>%
  filter(rejection == "Rejected") %>%
  ggplot(aes(x = beta1, y = proportion)) + 
    geom_point(aes(color = "viridis"), show.legend = FALSE) + 
    geom_line(aes(color = "viridis"), show.legend = FALSE) +
    labs(
      title = "Association Between Effect Size and Power",
      x = "Beta1 True Value",
      y = "Proportion that the Null was Rejected")  +
    viridis::scale_color_viridis(discrete = TRUE) 
```

![](p8105_hw5_niz2000_files/figure-gfm/Effect%20Size%20and%20Power-1.png)<!-- -->

``` r
results
```

    ## # A tibble: 70,000 x 6
    ##    beta1 term  estimate std.error statistic p.value
    ##    <dbl> <chr>    <dbl>     <dbl>     <dbl>   <dbl>
    ##  1     0 x       -0.912      1.31    -0.698  0.491 
    ##  2     0 x        2.11       1.10     1.91   0.0665
    ##  3     0 x        0.813      1.12     0.727  0.473 
    ##  4     0 x        2.03       1.24     1.64   0.113 
    ##  5     0 x        1.06       1.02     1.04   0.307 
    ##  6     0 x       -1.48       1.12    -1.32   0.196 
    ##  7     0 x        0.670      1.17     0.571  0.573 
    ##  8     0 x       -1.24       1.63    -0.764  0.451 
    ##  9     0 x        0.223      1.42     0.157  0.876 
    ## 10     0 x        0.221      1.21     0.183  0.856 
    ## # … with 69,990 more rows

In order to distinguish which p-values represent a value that would
reject or fail to reject the null hypothesis, a new variable “Rejection”
was created based on the cut-off 0.05. P-values less than 0.05 reject
the null, and p-values greater than 0.05 fail to reject the null. A
proportion was made to display the proportion of times that the null was
rejected. As evidenced by the resulting plot, there is a positive
relationship between effect size and power. As the true β1 increases,
power increases, following a sigmoid population growth curve. The slope
of the curve increases from flatter, to steep, and then appears to begin
to plateu when the true β1 is approximatley 6.

## Plot Showing the Average Estimate and True Value of Beta 1

This plot displays the average estimate of β̂1 on the y-axis compared to
the true value of β1 on the x-axis. Overlayed on this plot is another
plot comparing the average estimate and true value of β1, filtered among
samples for which the null was rejcted.

``` r
plot1 = results %>%
  group_by(beta1) %>%
  summarise(mean_estimate = mean(estimate))
plot1
```

    ## # A tibble: 7 x 2
    ##   beta1 mean_estimate
    ##   <dbl>         <dbl>
    ## 1     0       -0.0221
    ## 2     1        0.996 
    ## 3     2        1.99  
    ## 4     3        3.01  
    ## 5     4        3.99  
    ## 6     5        4.97  
    ## 7     6        6.00

``` r
plot2 = results %>%
  mutate(rejection = cut(p.value, breaks=c(0, 0.05, Inf), 
    labels=c("Rejected", "Not_Rejected"))) %>%
  filter(rejection == "Rejected") %>%
  group_by(beta1) %>%
  summarise(mean_estimate = mean(estimate))
plot2
```

    ## # A tibble: 7 x 2
    ##   beta1 mean_estimate
    ##   <dbl>         <dbl>
    ## 1     0       -0.0686
    ## 2     1        2.96  
    ## 3     2        3.39  
    ## 4     3        3.83  
    ## 5     4        4.42  
    ## 6     5        5.16  
    ## 7     6        6.06

``` r
ggplot(plot1, aes(x = beta1, y = mean_estimate)) + 
  geom_point(aes(color = "Overall")) + 
    geom_line(aes(color = "Overall")) +
  geom_point(data = plot2, aes(x = beta1, y = mean_estimate, color = "When Null Was Rejected")) +
    geom_line(data = plot2, aes(x = beta1, y = mean_estimate, color = "When Null Was Rejected")) +
  labs(
    title = "Average vs True Estimate of β1",
    x = "True Value of β 1",
    y = "Average Estimate of β̂ 1",
    color = "") +
    viridis::scale_color_viridis(discrete = TRUE) +
  theme(plot.title = element_text(hjust = 0.5))
```

![](p8105_hw5_niz2000_files/figure-gfm/Average%20and%20True%20Beta%201-1.png)<!-- -->

As displayed in the plot, the sample average of β̂1 across tests for
which the null is rejected differs from the true value of β1. Apart from
when the true β1 is 0 and 6, the sample average of β̂1 is greater than
the true value of β1, with the greatest divergence when the true value
of β1 is 1.

The sample average of β̂1 across tests for which the null is rejected is
not approximately equal to the true value of β1, which is what we would
expect. As we have filtered our estimates to only include significant
p-values based on our regression simulation, these are the values that
were significantly different from the true values of β1. As we wouldn’t
expect these filtered individual β̂1 estimates to be close to the true
β1, we also don’t expect that the average of those β̂1 estimates would
be significantly close to the true β1.
